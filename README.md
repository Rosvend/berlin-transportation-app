# Real-Time Public Transport Data Pipeline

---

## Purpose

The purpose of this project is to build a modern, production-grade real-time data pipeline that extracts, stores, transforms, and visualizes live public transport data from Berlinâ€™s BVG system using the [v6.bvg.transport.rest](https://v6.bvg.transport.rest) API. The pipeline is designed to be modular, cloud-portable (AWS-ready), and suitable for a data engineering portfolio.

---

## Scope

This project covers the **end-to-end lifecycle** of a real-time data pipeline:
- API data extraction
- Ingestion into a raw zone
- Loading into Snowflake
- Transformation with dbt
- DAG orchestration with Airflow
- Dashboard visualization
- CI/CD and automated data quality testing

---

## Tech Stack

| Layer             | Technology (Local)              | Cloud Equivalent (AWS)             |
|------------------|----------------------------------|------------------------------------|
| Ingestion         | Python + Airflow (Docker)        | MWAA / ECS / EKS                   |
| Storage           | MinIO or local filesystem        | Amazon S3                          |
| Warehouse         | Snowflake (Free Trial)           | Snowflake (Cloud)                  |
| Transformation    | dbt + Snowflake SQL              | dbt Cloud + Snowflake              |
| Orchestration     | Airflow (Docker Compose)         | MWAA / Airflow on ECS              |
| Stream (Optional) | Kafka + PySpark (simulated)      | MSK or Kinesis + Glue / EMR        |
| Visualization     | Streamlit or Jupyter             | EC2, Fargate, S3 + CloudFront      |
| CI/CD             | GitHub Actions                   | GitHub Actions / CodePipeline      |
| Data Quality      | Great Expectations + Pytest      | Same (CI/CD + S3, Snowflake)       |

---

## ğŸ—‚ï¸ Repo Structure

```bash
berlin-transport-pipeline/
â”œâ”€â”€ airflow/                  # Airflow DAGs and configurations
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ ingest_departure.py
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ config/                   # Configuration files
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ dashboards/               # Streamlit dashboards
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ data/                     # Local raw storage (MinIO mount)
â”‚   â”œâ”€â”€ mart/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ dbt/                      # dbt project
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ marts/
â”‚       â””â”€â”€ staging/
â”œâ”€â”€ docker/                   # Dockerfiles and initialization scripts
â”‚   â”œâ”€â”€ dockerfile.airflow
â”‚   â”œâ”€â”€ dockerfile.streamlit
â”‚   â””â”€â”€ init-airflow.sh
â”œâ”€â”€ extract/                  # API data fetch logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ departures.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ logs/                     # Logs directory
â”œâ”€â”€ notebooks/                # Exploration notebooks
â”‚   â””â”€â”€ EDA.ipynb
â”œâ”€â”€ scripts/                  # Manual tests, utilities
â”‚   â”œâ”€â”€ bucket_creation.sh
â”‚   â””â”€â”€ setup.sh
â”œâ”€â”€ tests/                    # Pytest unit + integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_departures.py
â”œâ”€â”€ transform/                # Transformation logic
â”œâ”€â”€ docker-compose.yml        # Orchestration of local stack
â”œâ”€â”€ makefile                  # Makefile for convenience commands
â”œâ”€â”€ requirements-streamlit.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                      # Secrets + credentials
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ .gitignore                # Git ignore file
```

---

### Prerequisites

**Install Docker and Docker Compose**:
- [Docker Installation Guide](https://docs.docker.com/get-docker/)
- [Docker Compose Installation Guide](https://docs.docker.com/compose/install/)

## How to Run in 5 simple steps

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/rosvend/berlin-transport-elt-snowflake.git
   cd berlin-transport-elt-snowflake
   ```

2. **Set Up Environment Variables**:
   - Copy `.env.template` to `.env`:
     ```bash
     cp .env.template .env
     ```
   - Update `.env` with your credentials (e.g., Snowflake, MinIO).

3. **Run setup script**:
   ```bash
     bash scripts/setup.sh
   ```

4. **Build and Start Services**:
   ```bash
   make up
   ```

5. **Access Services**:
   - **Airflow UI**: [http://localhost:8080](http://localhost:8080) (admin/admin)
   - **MinIO Console**: [http://localhost:9001](http://localhost:9001) (minioadmin/minioadmin123)
   - **Streamlit Dashboard**: [http://localhost:8501](http://localhost:8501)

### Useful Commands

- **Stop Services**:
  ```bash
  make down
  ```

- **View Logs**:
  ```bash
  make logs
  ```

- **Clean Up Containers and Volumes**:
  ```bash
  make clean
  ```

- **Check Service Health**:
  ```bash
  make health
  ```

---

## Next Steps

1. **Run Airflow DAGs**:
   - Navigate to the Airflow UI and trigger the DAGs.

2. **Explore Data**:
   - Use the Streamlit dashboard to visualize transport data.

3. **Extend the Pipeline**:
   - Add new DAGs, dbt models, or dashboards as needed.

---

## Troubleshooting

- **Airflow UI Not Accessible**:
  - Ensure the containers are running:
    ```bash
    docker-compose ps
    ```
  - Rebuild and restart services:
    ```bash
    make up
    ```

- **MinIO Buckets Not Created**:
  - Run the bucket creation script:
    ```bash
    make create-buckets
    ```

- **Streamlit Dashboard Not Loading**:
  - Check the logs:
    ```bash
    make logs
    ```
