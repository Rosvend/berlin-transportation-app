# Real-Time Public Transport Data Pipeline

---

## Purpose

The purpose of this project is to build a modern, production-grade real-time data pipeline that extracts, stores, transforms, and visualizes live public transport data from Berlinâ€™s BVG system using the [v6.bvg.transport.rest](https://v6.bvg.transport.rest) API. The pipeline is designed to be modular, cloud-portable (AWS-ready), and suitable for a data engineering portfolio.

---

## Scope

This project covers the **end-to-end lifecycle** of a real-time data pipeline:
- API data extraction
- Ingestion into a raw zone
- Loading into Snowflake
- Transformation with dbt
- DAG orchestration with Airflow
- Dashboard visualization
- CI/CD and automated data quality testing

---

## Tech Stack

| Layer             | Technology (Local)              | Cloud Equivalent (AWS)             |
|------------------|----------------------------------|------------------------------------|
| Ingestion         | Python + Airflow (Docker)        | MWAA / ECS / EKS                   |
| Storage           | MinIO or local filesystem        | Amazon S3                          |
| Warehouse         | Snowflake (Free Trial)           | Snowflake (Cloud)                  |
| Transformation    | dbt + Snowflake SQL              | dbt Cloud + Snowflake              |
| Orchestration     | Airflow (Docker Compose)         | MWAA / Airflow on ECS              |
| Stream (Optional) | Kafka + PySpark (simulated)      | MSK or Kinesis + Glue / EMR        |
| Visualization     | Streamlit or Jupyter             | EC2, Fargate, S3 + CloudFront      |
| CI/CD             | GitHub Actions                   | GitHub Actions / CodePipeline      |
| Data Quality      | Great Expectations + Pytest      | Same (CI/CD + S3, Snowflake)       |

---

## ğŸ—‚ï¸ Repo Structure

```bash
berlin-transport-pipeline/
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ ingest_departures.py
â”œâ”€â”€ extract/                   # API data fetch logic
â”‚   â””â”€â”€ departures.py
â”œâ”€â”€ transform/                 # dbt project lives here
â”‚   â””â”€â”€ dbt_project/
â”œâ”€â”€ config/                    # Configuration files
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ scripts/                   # Manual tests, utilities
â”‚   â””â”€â”€ test_connection.py
â”œâ”€â”€ tests/                     # Pytest unit + integration tests
â”œâ”€â”€ notebooks/                 # Exploration notebooks (optional)
â”œâ”€â”€ dashboard/                 # Streamlit dashboards
â”‚   â””â”€â”€ delays_dashboard.py
â”œâ”€â”€ data/                      # Local raw storage (MinIO mount)
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ docker-compose.yml         # Orchestration of local stack
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                       # Secrets + credentials
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
